{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ecaa0f4",
   "metadata": {},
   "source": [
    "# Delayed Monetary Losses: A Comparison of Two Procedures\n",
    "\n",
    "This notebook replicates the analyses from the publication:\n",
    "\n",
    "> Wan, H., Green, L., & Myerson, J. (2024). Delayed monetary losses: Do different procedures and measures assess the same construct?. *Behavioural Processes*, 105101. https://doi.org/10.1016/j.beproc.2024.105101\n",
    "\n",
    "The original analyses were conducted in R and have been translated into a Python workflow using `pandas`, `numpy`, `statsmodels`, `lmfit`, and `seaborn`. The goal is to determine whether the Adjusting-Amount and the Delayed Losses Questionnaire (DLQ) procedures assess the same underlying construct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7d0413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas numpy scipy lmfit statsmodels seaborn scikit-learn openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4accc070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup: Imports and Custom Functions ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from lmfit import Model\n",
    "from sklearn.metrics import auc as auc_calc\n",
    "from scipy.stats import gmean\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "# --- Custom Functions ---\n",
    "def set_mattheme(ax):\n",
    "    \"\"\"Applies a consistent plot theme similar to the R version.\"\"\"\n",
    "    ax.set_title(ax.get_title(), fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel(ax.get_xlabel(), fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel(ax.get_ylabel(), fontsize=14, fontweight='bold')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor('black')\n",
    "        spine.set_linewidth(1)\n",
    "    ax.set_facecolor('white')\n",
    "    sns.despine(ax=ax)\n",
    "\n",
    "def fit_nls_k(df):\n",
    "    \"\"\"Fits a simple hyperbola to individual Adj-Amt data to get log k.\"\"\"\n",
    "    def model_func(iv, k):\n",
    "        return 1 / (1 + np.exp(k) * iv)\n",
    "    \n",
    "    model = Model(model_func)\n",
    "    try:\n",
    "        result = model.fit(df['value'], iv=df['iv'], k=-4)\n",
    "        return result.params['k'].value\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# --- Data Loading and Processing ---\n",
    "\n",
    "# Load and filter raw data\n",
    "disc_raw = pd.read_csv(\"R Code/Data/DelayLoss.csv\", index_col=0)\n",
    "disc_df = disc_raw[\n",
    "    (disc_raw['check'] == 7) & (disc_raw['provider'] != \"MTurk\")\n",
    "].copy()\n",
    "\n",
    "# The original 'id' column has the participant identifiers.\n",
    "# We group by this column and use .ngroup() to create a new, sequential ID from 1 to N,\n",
    "# which matches the logic from the original R script.\n",
    "disc_df['id'] = disc_df.groupby('id').ngroup() + 1\n",
    "\n",
    "# Drop the now-unnecessary 'provider' column\n",
    "disc_df = disc_df.drop(columns=['provider'])\n",
    "\n",
    "# --- Process Adj-Amt Data ---\n",
    "adj_amt_processed = disc_df[disc_df['procedure'] == \"aa\"].groupby(['id', 'procedure', 'amt']).apply(\n",
    "    lambda g: pd.Series({\n",
    "        'atheoretical': auc_calc(g['iv'] / 108, g['value']),\n",
    "        'k': fit_nls_k(g)\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# --- Process MCQ Data ---\n",
    "def score_mcq_logk(df):\n",
    "    \"\"\"\n",
    "    Calculates theoretical log k for the Delayed Losses Questionnaire.\n",
    "    The logic is now consistent with the delayed gains scoring method.\n",
    "    \"\"\"\n",
    "    choices = df['value'].values\n",
    "    k_values = df['iv'].values\n",
    "    \n",
    "    # Define questionnaire k-value boundaries for edge cases\n",
    "    min_k = 9e-6  # Smallest k in the questionnaire design\n",
    "    max_k = 0.014 # Largest k in the questionnaire design\n",
    "    \n",
    "    # Handle edge cases\n",
    "    # Always chooses immediate loss (value=1) -> patient for losses -> smallest k\n",
    "    if all(choices == 1): \n",
    "        return np.log(min_k)\n",
    "    # Always chooses delayed loss (value=0) -> impulsive for losses -> largest k\n",
    "    if all(choices == 0): \n",
    "        return np.log(max_k)\n",
    "    \n",
    "    # For participants who switch, find k that maximizes response consistency\n",
    "    n_consistent = [sum((choices == 0) & (k_values <= k) | (choices == 1) & (k_values >= k)) for k in k_values]\n",
    "    max_consistency = np.max(n_consistent)\n",
    "    indifference_ks = k_values[np.where(n_consistent == max_consistency)]\n",
    "    \n",
    "    # Return log of the geometric mean, consistent with your preferred calculation\n",
    "    return np.log(np.exp(np.mean(np.log(indifference_ks))))\n",
    "\n",
    "mcq_processed = disc_df[disc_df['procedure'] == \"mcq\"].groupby(['id', 'amt']).apply(\n",
    "    lambda g: pd.Series({\n",
    "        'atheoretical': g['value'].sum(),\n",
    "        'k': score_mcq_logk(g)\n",
    "    })\n",
    ").reset_index()\n",
    "mcq_processed['procedure'] = 'mcq'\n",
    "\n",
    "# --- Combine into Final Analysis Dataframe ---\n",
    "behav = pd.concat([adj_amt_processed, mcq_processed]).sort_values(['id', 'procedure', 'amt']).reset_index(drop=True)\n",
    "\n",
    "# --- Create Choice Pattern Dataframes ---\n",
    "def get_grp(df):\n",
    "    corr = df['value'].corr(np.log(df['iv']))\n",
    "    mean_val = df['value'].mean()\n",
    "    if pd.isna(corr): corr = 0\n",
    "    \n",
    "    if df['procedure'].iloc[0] == 'aa':\n",
    "        if corr < 0 or mean_val == df['value'].min(): return \"Typical\"\n",
    "        if corr > 0: return \"Atypical\"\n",
    "        if mean_val == df['value'].max(): return \"Always immediate\"\n",
    "    else: # mcq\n",
    "        if corr > 0 or mean_val == 0: return \"Typical\"\n",
    "        if corr < 0: return \"Atypical\"\n",
    "        if mean_val == 1: return \"Always immediate\"\n",
    "    return \"Typical\"\n",
    "\n",
    "chc_pat_raw = disc_df[((disc_df['procedure'] == 'aa') & (disc_df['amt'] != 3)) |\n",
    "                      ((disc_df['procedure'] == 'mcq') & (disc_df['amt'] != 2))\n",
    "                     ].groupby(['procedure', 'id', 'amt']).apply(get_grp).reset_index(name='grp')\n",
    "\n",
    "chc_pat = chc_pat_raw.groupby(['procedure', 'id'])['grp'].apply(\n",
    "    lambda g: g.iloc[0] if g.nunique() == 1 else \"Undefined\"\n",
    ").reset_index()\n",
    "\n",
    "co = chc_pat.pivot(index='id', columns='procedure', values='grp')\n",
    "\n",
    "# --- Group Level Data Frame ---\n",
    "disc_grp_df = disc_df.copy()\n",
    "disc_grp_df.loc[disc_grp_df['procedure'] == 'mcq', 'iv'] = np.log(disc_grp_df.loc[disc_grp_df['procedure'] == 'mcq', 'iv'])\n",
    "disc_grp_df = disc_grp_df.groupby(['procedure', 'amt', 'iv']).agg(\n",
    "    mean_sv=('value', 'mean'), med_sv=('value', 'median')\n",
    ").reset_index().sort_values('procedure')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1d3325",
   "metadata": {},
   "source": [
    "## Group-Level Analyses\n",
    "\n",
    "This section visualizes the group-level discounting functions for both the Adjusting-Amount and DLQ procedures and reports the goodness-of-fit for the corresponding nonlinear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b48b687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Adj-Amt: R-squared ---\n",
      "amt\n",
      "1   0.953\n",
      "2   0.953\n",
      "3   0.970\n",
      "dtype: float64\n",
      "\n",
      "--- DLQ: R-squared ---\n",
      "amt\n",
      "1   0.984\n",
      "2   0.984\n",
      "3   0.976\n",
      "dtype: float64\n",
      "--- Adj-Amt: Amount Effect (Large vs. Small) ---\n",
      "                             Test for Constraints                             \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "c0            -0.2510      0.146     -1.723      0.085      -0.537       0.035\n",
      "==============================================================================\n",
      "\n",
      "--- DLQ: Amount Effect (Large vs. Small) ---\n",
      "                             Test for Constraints                             \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "c0            -0.0856      0.142     -0.601      0.548      -0.365       0.194\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Goodness-of-Fit (R-squared) ---\n",
    "def hyperboloid_loss_fit(df):\n",
    "    def model_func(iv, k, b): return 1 / (1 + np.exp(k) * iv)**b\n",
    "    model = Model(model_func)\n",
    "    return model.fit(df['med_sv'], iv=df['iv'], k=-4, b=1).rsquared\n",
    "\n",
    "def logistic_fit(df):\n",
    "    def model_func(iv, x, r): return 1 / (1 + np.exp(-(df['iv'] - x) * r))\n",
    "    model = Model(model_func)\n",
    "    return model.fit(df['mean_sv'], iv=df['iv'], x=-10, r=1).rsquared\n",
    "\n",
    "print(\"--- Adj-Amt: R-squared ---\")\n",
    "r2_aa = disc_grp_df[disc_grp_df['procedure'] == 'aa'].groupby('amt').apply(hyperboloid_loss_fit)\n",
    "print(r2_aa)\n",
    "\n",
    "print(\"\\n--- DLQ: R-squared ---\")\n",
    "r2_mcq = disc_grp_df[disc_grp_df['procedure'] == 'mcq'].groupby('amt').apply(logistic_fit)\n",
    "print(r2_mcq)\n",
    "\n",
    "# --- Amount Effect Test ---\n",
    "print(\"--- Adj-Amt: Amount Effect (Large vs. Small) ---\")\n",
    "aa_data = behav[behav['procedure'] == 'aa'].copy()\n",
    "aa_data['amt'] = aa_data['amt'].astype('category')\n",
    "# GLM with Binomial family can handle proportional data\n",
    "aa_data['atheoretical_adj'] = np.clip(aa_data['atheoretical'], 1e-5, 1 - 1e-5)\n",
    "glm_beta_model = smf.glm(\"atheoretical_adj ~ -1 + C(amt)\", data=aa_data, \n",
    "                         family=sm.families.Binomial()).fit()\n",
    "print(glm_beta_model.t_test(\"C(amt)[3] - C(amt)[1] = 0\"))\n",
    "\n",
    "\n",
    "print(\"\\n--- DLQ: Amount Effect (Large vs. Small) ---\")\n",
    "mcq_data = behav[behav['procedure'] == 'mcq'].copy()\n",
    "mcq_data['amt'] = mcq_data['amt'].astype('category')\n",
    "# Create proportion and provide total trials as weights\n",
    "mcq_data['prop_immediate'] = mcq_data['atheoretical'] / 9.0\n",
    "mcq_data['n_trials'] = 9\n",
    "glm_binomial_model = smf.glm(\"prop_immediate ~ -1 + C(amt)\", data=mcq_data, \n",
    "                             family=sm.families.Binomial(),\n",
    "                             weights=mcq_data['n_trials']).fit()\n",
    "print(glm_binomial_model.t_test(\"C(amt)[3] - C(amt)[1] = 0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a3f69",
   "metadata": {},
   "source": [
    "## Individual-Level Discounting Measures\n",
    "\n",
    "This section examines the reliability of the discounting measures (i.e., correlations between amounts *within* each procedure) and their construct validity (i.e., correlations *between* the two procedures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4bb58ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Reliability: Correlations Between Amounts ---\n",
      "amt                            1     2     3\n",
      "procedure measure      amt                  \n",
      "aa        atheoretical 1   1.000 0.782 0.636\n",
      "                       2   0.782 1.000 0.763\n",
      "                       3   0.636 0.763 1.000\n",
      "          k            1   1.000 0.797 0.627\n",
      "                       2   0.797 1.000 0.763\n",
      "                       3   0.627 0.763 1.000\n",
      "mcq       atheoretical 1   1.000 0.866 0.806\n",
      "                       2   0.866 1.000 0.893\n",
      "                       3   0.806 0.893 1.000\n",
      "          k            1   1.000 0.836 0.747\n",
      "                       2   0.836 1.000 0.840\n",
      "                       3   0.747 0.840 1.000\n",
      "\n",
      "--- Reliability: Correlations Between Measures (Atheoretical vs. Theoretical) ---\n",
      "procedure  amt\n",
      "aa         1     -0.960\n",
      "           2     -0.968\n",
      "           3     -0.967\n",
      "mcq        1     -0.981\n",
      "           2     -0.982\n",
      "           3     -0.977\n",
      "Name: (atheoretical, k), dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- Reliability Analysis ---\n",
    "print(\"--- Reliability: Correlations Between Amounts ---\")\n",
    "behav_long = behav.melt(id_vars=['id', 'procedure', 'amt'], value_vars=['atheoretical', 'k'],\n",
    "                        var_name='measure', value_name='score')\n",
    "reliability_pivot = behav_long.pivot_table(index=['id', 'procedure', 'measure'], columns='amt', values='score')\n",
    "print(reliability_pivot.groupby(['procedure', 'measure']).corr())\n",
    "\n",
    "\n",
    "print(\"\\n--- Reliability: Correlations Between Measures (Atheoretical vs. Theoretical) ---\")\n",
    "print(behav.groupby(['procedure', 'amt'])[['atheoretical', 'k']].corr().unstack().iloc[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35d56e",
   "metadata": {},
   "source": [
    "## Analysis of Choice Patterns\n",
    "\n",
    "This section categorizes participants based on their response patterns (e.g., typical, always immediate, atypical) and examines the consistency of these patterns across the two procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "73573493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Frequencies of Choice Patterns by Procedure ---\n",
      "procedure  grp             \n",
      "aa         Typical            0.877\n",
      "           Undefined          0.104\n",
      "           Atypical           0.019\n",
      "mcq        Typical            0.759\n",
      "           Always immediate   0.155\n",
      "           Undefined          0.084\n",
      "           Atypical           0.002\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- Contingency Table of Choice Patterns (Adj-Amt vs. DLQ) ---\n",
      "mcq        Always immediate  Atypical  Typical  Undefined\n",
      "aa                                                       \n",
      "Atypical                  2         0        5          1\n",
      "Typical                  59         1      288         30\n",
      "Undefined                 6         0       34          5\n",
      "\n",
      "--- Cross-Tabulation with Expected Frequencies ---\n",
      "Expected Frequencies:\n",
      "mcq        Always immediate  Atypical  Typical  Undefined\n",
      "aa                                                       \n",
      "Atypical              1.244     0.019    6.070      0.668\n",
      "Typical              58.761     0.877  286.789     31.573\n",
      "Undefined             6.995     0.104   34.142      3.759\n"
     ]
    }
   ],
   "source": [
    "# --- Choice Pattern Analysis ---\n",
    "print(\"--- Frequencies of Choice Patterns by Procedure ---\")\n",
    "print(chc_pat.groupby('procedure')['grp'].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\n--- Contingency Table of Choice Patterns (Adj-Amt vs. DLQ) ---\")\n",
    "crosstab = pd.crosstab(co['aa'], co['mcq'])\n",
    "print(crosstab)\n",
    "\n",
    "print(\"\\n--- Cross-Tabulation with Expected Frequencies ---\")\n",
    "chi2, p, dof, expected = chi2_contingency(crosstab)\n",
    "print(\"Expected Frequencies:\")\n",
    "print(pd.DataFrame(expected, index=crosstab.index, columns=crosstab.columns))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
