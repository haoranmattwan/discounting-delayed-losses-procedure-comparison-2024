---
title: "Validating Efficient Methods for Measuring Intertemporal Choice"
subtitle: "A Methodological Comparison of Two Procedures for Assessing Delayed Losses"
author: "Haoran (Matt) Wan"
date: "today"
format: 
  html:
    toc: true
    code-fold: false
    self-contained: true
    theme: cosmo
    mainfont: "Garamond"
engine: knitr
---

## Introduction

This document provides the complete R code to replicate the analyses from the publication:

> Wan, H., Green, L., & Myerson, J. (2024). Delayed monetary losses: Do different procedures and measures assess the same construct?. *Behavioural Processes, 222*, 105101. [https://doi.org/10.1016/j.beproc.2024.105101](https://doi.org/10.1016/j.beproc.2024.105101)

### Project Objective
The primary goal of this study is **methodological validation**. It formally tests whether a brief, 27-item survey (the Delayed Losses Questionnaire, or DLQ) provides a valid and reliable measure of delay discounting when compared to the more resource-intensive, iterative Adjusting-Amount (Adj-Amt) procedure.

### Analysis Workflow
1.  **Setup**: Loads all necessary packages and defines custom functions.
2.  **Data Processing**: Cleans and transforms the raw data, calculating four key discounting measures (two for each procedure).
3.  **Group-Level Analysis**: Visualizes aggregate choice patterns and fits non-linear models to assess data quality.
4.  **Reliability Analysis**: Examines the internal consistency of measures *within* each procedure.
5.  **Validity Analysis**: Tests the core hypothesis by correlating measures *between* the two procedures.
6.  **Choice Pattern Analysis**: Categorizes individual response styles and assesses their consistency across procedures.

The data used in this analysis are publicly available on the Open Science Framework at [https://osf.io/emb2q/](https://osf.io/emb2q/).

```{r setup}
#| message: false
#| warning: false

# --- 1. SETUP: PACKAGES, OPTIONS, AND FUNCTIONS ---

# Set global chunk options to suppress messages and warnings for a clean HTML output
knitr::opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE)

# --- Load Packages ---
# Modeling & Statistics
library(minpack.lm) # For non-linear least squares fitting
library(betareg)    # For beta regression
library(multcomp)   # For general linear hypotheses
library(gmodels)    # For cross-tabulation

# Visualization
library(ggplot2)
library(ggpubr)
library(lemon)

# Data Wrangling & Core Utilities
library(tidyverse)
library(here)
library(janitor)

# --- Custom Functions ---

#' Custom ggplot2 Theme for Publication-Ready Plots
#' @description A standardized theme for consistent plot aesthetics.
#' @return A ggplot theme object.
mattheme <- theme(
  text = element_text(size = 12, family = "Arial", color = "black", face = "bold"), 
  axis.text.y = element_text(colour = "black", size = 10, face = "bold"), 
  axis.text.x = element_text(colour = "black", size = 10, face = "bold", angle = 0), 
  axis.title.x = element_text(margin = margin(7, 0, 0, 0), size = 14), 
  axis.title.y = element_text(margin = margin(0, 7, 0, 0), size = 14), 
  plot.title = element_text(size = 14, face = "bold", hjust = 0.5), 
  panel.background = element_rect(fill = "white", linetype = 1, color = "black", size = 1), 
  panel.grid = element_blank(),
  strip.background = element_blank(),
  strip.text = element_text(size=10),
  legend.key = element_blank()
)

#' Area Under the Curve (AuC) Calculation
#' @description Calculates area under the curve using the trapezoidal rule.
#' @param x A numeric vector of x-coordinates (e.g., normalized delays).
#' @param y A numeric vector of y-coordinates (e.g., subjective values).
#' @return The calculated area as a single numeric value.
auc <- function(x, y) {
  sum(diff(x) * (y[-1] + y[-length(y)])) / 2
}
```

## Data Processing

This section details the entire data processing pipeline. The raw data is loaded, filtered to the final sample (N = 431), and then transformed to calculate the four primary discounting scores:
- **For the Adj-Amt procedure**:
    1. `auc_adj_amt`: Area Under the Curve (atheoretical).
    2. `logk_adj_amt`: The log-transformed *k* parameter from a simple hyperbolic model (theoretical).
- **For the DLQ procedure**:
    1. `prop_imm_dlq`: The proportion of immediate choices (atheoretical).
    2. `logk_dlq`: The log-transformed *k* parameter estimated from the choice pattern (theoretical).

Finally, individual choice patterns are classified for a subsequent consistency analysis.

```{r load_data}
#| message: false
#| warning: false

# --- 2. DATA LOADING AND PROCESSING ---

# --- Load and Clean Raw Data ---
setwd(here::here())
data_raw <- read_csv("R Code/Data/DelayLoss.csv") |> select(-1)

# Filter to final sample:
# - Remove participants who failed attention checks (check == 7)
# - Remove data from MTurk due to high failure rates as noted in the paper
data_clean <- data_raw |>
  filter(check == 7) |>
  filter(provider != "MTurk") |>
  select(-provider) |>
  # Re-assign sequential participant IDs after filtering
  mutate(id = rep(1:n_distinct(id), each = n_distinct(procedure, amt, iv)))

# --- Calculate Scores for the Adjusting-Amount (Adj-Amt) Procedure ---
adj_amt_scores <- data_clean |>
  filter(procedure == "aa") |>
  group_by(id, amt) |>
  summarise(
    # 1. Atheoretical measure: Area Under the Curve (AuC)
    auc_adj_amt = auc(iv / 108, value),
    # 2. Theoretical measure: log(k) from a simple hyperbola
    # CORRECTED: Fit the model for k directly and then log-transform the result.
    # This is more explicit and less prone to reparameterization errors.
    logk_adj_amt = tryCatch({
      fit <- nlsLM(value ~ 1 / (1 + k * iv), start = list(k = 0.01))
      log(coef(fit)[['k']])
    }, error = function(e) NA),
    .groups = 'drop'
  )

# --- Calculate Scores for the Delayed Losses Questionnaire (DLQ) ---
dlq_scores <- data_clean |>
  filter(procedure == "mcq") |>
  mutate(iv = case_when(
    iv == 0.00088 ~ 0.00089,
    iv == 0.00180 ~ 0.00230,
    iv %in% c(0.0058, 0.0059) ~ 0.00550,
    TRUE ~ iv
  )) |>
  group_by(id, amt) |>
  summarise(
    # 1. Atheoretical measure: Proportion of immediate choices (out of 9 items)
    prop_imm_dlq = mean(value),
    # 2. Theoretical measure: log(k) estimated from the choice pattern
    logk_dlq = {
      choices <- value
      k_values <- iv
      # Handle non-switching participants
      if (all(choices == 1)) {
        log(min(k_values)) # Always immediate -> largest k
      } else if (all(choices == 0)) {
        log(max(k_values)) # Always delayed -> smallest k
      } else {
        # Find k that maximizes response consistency around a switch point
        n_consistent <- sapply(k_values, function(k_level) {
          sum((choices == 0 & k_values <= k_level) | (choices == 1 & k_values >= k_level))
        })
        indifference_ks <- k_values[which(n_consistent == max(n_consistent))]
        log(prod(indifference_ks)^(1/length(indifference_ks))) # Geometric mean
      }
    },
    .groups = 'drop'
  )

# --- Combine All Scores into a Single Tidy Dataframe ---
scores_combined <- full_join(adj_amt_scores, dlq_scores, by = c("id", "amt"))

# --- Classify Individual Choice Patterns ---
adj_amt_patterns <- data_clean |>
  filter(procedure == "aa" & amt != 3) |>
  group_by(id, amt) |>
  summarise(correlation = cor(value, log(iv)), mean_sv = mean(value), .groups = 'drop') |>
  mutate(
    correlation = ifelse(is.na(correlation), 0, correlation),
    pattern = case_when(
      correlation < 0 | mean_sv == min(subset(data_clean, procedure == "aa")$value) ~ "Typical",
      correlation > 0 ~ "Atypical",
      mean_sv == max(subset(data_clean, procedure == "aa")$value) ~ "Always immediate"
      ),
    pattern = ifelse(is.na(pattern), "Typical", pattern)
  ) |>
  group_by(id) |>
  summarise(pattern_adj_amt = if_else(n_distinct(pattern) == 1, first(pattern), "Inconsistent"))

dlq_patterns <- data_clean |>
  filter(procedure == "mcq" & amt != 2) |>
  group_by(id, amt) |>
  summarise(correlation = cor(value, log(iv)), mean_choice = mean(value), .groups = 'drop') |>
  mutate(
    correlation = if_else(is.na(correlation), 0, correlation),
    pattern = case_when(
      correlation > 0 | mean_choice == 0 ~ "Typical",
      correlation < 0 ~ "Atypical",
      mean_choice == 1 ~ "Always immediate"
    )
  ) |>
  group_by(id) |>
  summarise(pattern_dlq = if_else(n_distinct(pattern) == 1, first(pattern), "Inconsistent"))

pattern_consistency <- full_join(adj_amt_patterns, dlq_patterns, by = "id")

# --- Create Group-Level Dataframe for Plotting (Figure 1) ---
data_group_level <- data_clean |>
  rename("plot_x" = "iv") |>
  group_by(procedure, amt, plot_x) |>
  summarise(mean_value = mean(value), .groups = 'drop')
```

## Group-Level Analysis

This section replicates Figure 1 from the publication, visualizing the aggregate discounting patterns for both procedures. The plots confirm that the data conform to established findings: subjective value decreases with delay (Adj-Amt), and the probability of choosing the immediate option increases with the question's *k* parameter (DLQ). Goodness-of-fit statistics confirm that the chosen non-linear models describe the data well.

```{r group_analysis}
#| message: false
#| warning: false

# --- 3. GROUP-LEVEL ANALYSIS (FIGURE 1 REPLICATION) ---

# --- Plot for Adjusting-Amount Procedure ---
plot_adj_amt <- data_group_level |>
  filter(procedure == "aa") |>
  mutate(amount_label = factor(amt, levels = c(1,2,3), labels = c("$90", "$240","$1,500"))) |>
  ggplot(aes(x = plot_x, y = mean_value, shape = amount_label, color = amount_label, linetype = amount_label)) +
  # Fit hyperboloid discounting functions to the group means
  geom_smooth(method = "nlsLM", formula = y ~ 1 / (1 + exp(k) * x)^s, 
              method.args = list(start = list(k = -4, s = 1)), se = FALSE, size = 0.8) +
  geom_point(aes(fill = amount_label), size = 4, stroke = 1) +
  scale_y_continuous(limits = c(0, 1.05), breaks = seq(0, 1, by = 0.2)) +
  scale_x_continuous(limits = c(0, 113), breaks = c(4, 18, 60, 108)) +
  scale_shape_manual(name = "Amount", values = c("$90"=21, "$240"=22, "$1,500"=24)) +
  scale_color_manual(name = "Amount", values = c("$90"="#1f78b4", "$240"="#ff7f00", "$1,500"="#33a02c")) +
  scale_fill_manual(name = "Amount", values = c("$90"="#1f78b4", "$240"="#ff7f00", "$1,500"="#33a02c")) +
  scale_linetype_manual(name = "Amount", values = c("$90"="solid", "$240"="dashed", "$1,500"="dotted")) +
  labs(x = "Delay (months)", y = "Relative Subjective Value", title = "Adjusting-Amount Procedure") +
  theme_bw() + mattheme + theme(legend.position = c(0.8, 0.25))

# --- Plot for Delayed Losses Questionnaire ---
plot_dlq <- data_group_level |>
  filter(procedure == "mcq") |>
  mutate(amount_label = factor(amt, levels = c(1,2,3), labels = c("$75-105", "$150-180","$225-255"))) |>
  ggplot(aes(x = plot_x, y = mean_value, shape = amount_label, color = amount_label, linetype = amount_label)) +
  # Fit logistic functions to the choice proportions
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE, size = 0.8) +
  geom_point(aes(fill = amount_label), size = 4, stroke = 1) +
  scale_x_continuous(trans = 'log10', breaks = c(1e-5, 1e-4, 1e-3, 1e-2), labels = c("0.00001", "0.0001", "0.001", "0.01")) +
  scale_y_continuous(limits = c(0, 1.05), breaks = seq(0, 1, by = 0.2)) +
  scale_shape_manual(name = "Amount", values = c("$75-105"=21, "$150-180"=22, "$225-255"=24)) +
  scale_color_manual(name = "Amount", values = c("$75-105"="#1f78b4", "$150-180"="#ff7f00", "$225-255"="#33a02c")) +
  scale_fill_manual(name = "Amount", values = c("$75-105"="#1f78b4", "$150-180"="#ff7f00", "$225-255"="#33a02c")) +
  scale_linetype_manual(name = "Amount", values = c("$75-105"="solid", "$150-180"="dashed", "$225-255"="dotted")) +
  labs(x = expression(paste(bold("Question "), bolditalic("k"), bold(" Parameter"))), y = "Choice of Immediate Payment", title = "Delayed Losses Questionnaire") +
  theme_bw() + mattheme + theme(legend.position = c(0.2, 0.75))

# --- Arrange Plots and Display ---
ggarrange(plot_adj_amt, plot_dlq, ncol = 2)

# --- Goodness-of-Fit (R-squared) for Group-Level Models ---
cat("--- Adj-Amt: R-squared for Hyperboloid Fits ---\n")
adj_amt_r2 <- data_group_level |>
  filter(procedure == "aa") |>
  group_by(amt) |>
  summarise(R2 = modelr::rsquare(
    nlsLM(mean_value ~ 1 / (1 + exp(k) * plot_x)^s, start = list(k = -4, s = 1)),
    data = pick(everything())
  ))
print(adj_amt_r2, digits = 3)

cat("\n--- DLQ:  R-squared for Logistic Fits ---\n")
dlq_r2 <- data_group_level |>
  filter(procedure == "mcq") |>
  group_by(amt) |>
  summarise(R2 = modelr::rsquare(
    nlsLM(mean_value ~ 1 / (1 + exp( - ( log(plot_x) - (x) ) * (r) ) ), 
    start = list(x = -10, r = 1), control = list(maxiter = 1000)),
    data = pick(everything())
  ))
print(dlq_r2, digits = 3)
```

## Reliability and Validity Analysis

This is the core of the study. First, **reliability** is assessed by examining the correlations between different loss amounts *within* each procedure. High correlations indicate that the procedure consistently measures the same construct regardless of the specific amounts used. Second, **convergent validity** is tested by examining the correlations *between* the two procedures. High correlations would provide strong evidence that both methods are measuring the same underlying construct.

```{r reliability_validity}
#| warning: false
#| message: false

# --- 4. INDIVIDUAL-LEVEL RELIABILITY AND VALIDITY ---

# --- Within-Procedure Reliability (Alternate-Forms) ---
# This assesses if the measures are consistent across different amounts.
cat("--- Reliability: Correlations Between Amounts Within Each Procedure ---\n")
reliability_table <- scores_combined |>
  pivot_longer(names_to = "measure", values_to = "score", cols = c(auc_adj_amt, logk_adj_amt, prop_imm_dlq, logk_dlq)) |>
  separate(measure, into = c("measure_type", "procedure"), sep = "_", extra = "merge") |>
  drop_na(score) |>
  pivot_wider(names_from = amt, values_from = score, names_prefix = "amount_") |>
  group_by(procedure, measure_type) |>
  summarise(
    Small_vs_Medium = cor(amount_1, amount_2, use = "pairwise.complete.obs"),
    Small_vs_Large = cor(amount_1, amount_3, use = "pairwise.complete.obs"),
    Medium_vs_Large = cor(amount_2, amount_3, use = "pairwise.complete.obs"),
    .groups = 'drop'
  )
print(reliability_table, digits = 3)

# --- Within-Procedure Concurrent Validity ---
# This assesses if the atheoretical and theoretical measures capture the same information.
# The correlation should be high and negative, as higher AuC/prop_imm means steeper
# discounting, while higher logk means shallower discounting.
cat("\n--- Concurrent Validity: Correlations Between Atheoretical and Theoretical Measures ---\n")
concurrent_validity_table <- scores_combined |>
  group_by(amt) |>
  summarise(
    Adj_Amt_Corr = cor(auc_adj_amt, logk_adj_amt, use = "pairwise.complete.obs"),
    DLQ_Corr = cor(prop_imm_dlq, logk_dlq, use = "pairwise.complete.obs"),
    .groups = 'drop'
  )
print(concurrent_validity_table, digits = 3)

# --- Between-Procedure Convergent Validity (Main Result) ---
# - Adj-Amt Amount 1 ($90) is matched with DLQ Amount 1 ($90 average).
# - Adj-Amt Amount 2 ($240) is matched with DLQ Amount 3 ($240 average).
cat("\n--- Convergent Validity: Correlations Between Procedures (Main Result) ---\n")
# Create separate wide-format data for each procedure
adj_amt_wide <- adj_amt_scores |>
  pivot_wider(id_cols = id, names_from = amt, values_from = c(auc_adj_amt, logk_adj_amt))

dlq_wide <- dlq_scores |>
  pivot_wider(id_cols = id, names_from = amt, values_from = c(prop_imm_dlq, logk_dlq))

# Join them into a single wide dataframe for easy correlation
validity_data_wide <- left_join(adj_amt_wide, dlq_wide, by = "id")

# Calculate the correlations for the matched amounts
convergent_validity_results <- tibble(
  Amount_Match = c("$90 vs $90", "$240 vs $240"),
  Atheoretical_Corr = c(
    cor(validity_data_wide$auc_adj_amt_1, validity_data_wide$prop_imm_dlq_1, use = "pairwise.complete.obs"),
    cor(validity_data_wide$auc_adj_amt_2, validity_data_wide$prop_imm_dlq_3, use = "pairwise.complete.obs")
  ),
  Theoretical_Corr = c(
    cor(validity_data_wide$logk_adj_amt_1, validity_data_wide$logk_dlq_1, use = "pairwise.complete.obs"),
    cor(validity_data_wide$logk_adj_amt_2, validity_data_wide$logk_dlq_3, use = "pairwise.complete.obs")
  )
)
print(convergent_validity_results, digits = 3)
```

## Analysis of Choice Patterns

Finally, this section examines whether individuals show consistent *qualitative* patterns of choice across the two procedures. Participants are classified based on their response styles, and a contingency table analysis is used to test if a participant's classification on one procedure is significantly associated with their classification on the other.

```{r choice_pattern}
#| warning: false
#| message: false

# --- 5. ANALYSIS OF CHOICE PATTERNS ---

# --- Frequencies of Choice Patterns by Procedure ---
cat("--- Frequencies of Choice Patterns by Procedure ---\n")
pattern_summary <- pattern_consistency |>
  pivot_longer(cols = c(pattern_adj_amt, pattern_dlq), names_to = "procedure", values_to = "pattern") |>
  mutate(procedure = str_remove(procedure, "pattern_")) |>
  group_by(procedure) |>
  count(pattern) |>
  mutate(proportion = n / sum(n))
print(pattern_summary, n=10)

# --- Contingency Table of Choice Patterns (Adj-Amt vs. DLQ) ---
# This cross-tabulation shows how many participants were classified in each
# combination of categories across the two procedures.
cat("\n--- Cross-Tabulation of Choice Patterns (Observed Counts) ---\n")
contingency_table <- table(
  `Adj-Amt` = pattern_consistency$pattern_adj_amt, 
  `DLQ` = pattern_consistency$pattern_dlq
)
print(contingency_table)

# --- Formal Chi-Squared Test for Association ---
# This tests if the number of people showing consistent patterns is greater than chance.
cat("\n--- Cross-Tabulation with Chi-Squared Test ---\n")
CrossTable(
  pattern_consistency$pattern_adj_amt, 
  pattern_consistency$pattern_dlq,
  expected = TRUE, 
  chisq = TRUE, 
  format = "SPSS"
)
```